#!/Users/bytedance/codes/lora-finetune/.venv/bin/python3
"""
åŠŸèƒ½ï¼šåŠ è½½Qwen-1.5-1.8Bï¼Œç”¨650æ¡æ•°æ®åšLoRAå¾®è°ƒï¼Œæ”¯æŒcheckpointç»­è·‘ï¼Œä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šç›®å½•
æ‰§è¡Œæ–¹å¼ï¼špython3 02_finetune.py
ä¾èµ–ï¼šéœ€å…ˆæ‰§è¡Œ01_generate_data.pyç”Ÿæˆæ•°æ®ï¼Œä¸”å®‰è£…ä¾èµ–ï¼ˆpip3 install torch transformers datasets peft accelerateï¼‰
"""
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model

def main():
    # ===================== 1. é…ç½®å‚æ•°ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰ =====================
    MODEL_NAME = "Qwen/Qwen1.5-1.8B"
    DATA_PATH = "/Users/bytedance/codes/lora-finetune/ocr_refinement_m3/data/650case.jsonl"
    OUTPUT_DIR = "/Users/bytedance/codes/lora-finetune/ocr_refinement_m3/model_finetuned_output"
    # æ–°å¢ï¼šæŒ‡å®šcheckpointæ¢å¤è·¯å¾„ï¼ˆè®¾ä¸ºNoneåˆ™è‡ªåŠ¨æ‰¾æœ€æ–°çš„ï¼‰
    RESUME_FROM_CHECKPOINT = None  # ä¹Ÿå¯ä»¥æŒ‡å®šå…·ä½“è·¯å¾„å¦‚ï¼šOUTPUT_DIR + "/checkpoint-50"

    # ===================== 2. åŠ è½½æ¨¡å‹å’ŒTokenizer =====================
    print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½Qwen-1.8Bï¼Œçº¦1.8GBï¼‰")
    # Mac CPUä¸“ç”¨é…ç½®ï¼šä¸é‡åŒ–ã€float32ã€device_map=cpu
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        torch_dtype=torch.float32,
        device_map="cpu"
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        padding_side="right"  # å³å¡«å……ï¼Œé¿å…å½±å“ç”Ÿæˆ
    )
    # Qwené»˜è®¤æ— pad_tokenï¼Œæ‰‹åŠ¨è®¾ç½®ä¸ºeos_token
    tokenizer.pad_token = tokenizer.eos_token

    # ===================== 3. é…ç½®LoRAï¼ˆæç®€ç‰ˆï¼ŒåŠ å¿«è®­ç»ƒï¼‰ =====================
    model = prepare_model_for_lora_training(model)  # ç®€åŒ–ç‰ˆprepareï¼Œé€‚é…CPU

    lora_config = LoraConfig(
        r=4,                
        lora_alpha=16,      # ç¼©æ”¾å› å­
        lora_dropout=0.05,  # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        target_modules=["q_proj", "v_proj"],  
        bias="none",
        task_type="CAUSAL_LM"  # å› æœè¯­è¨€æ¨¡å‹ï¼Œé€‚é…Qwen
    )

    model = get_peft_model(model, lora_config)
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ï¼ˆçº¦50ä¸‡ï¼Œæå¿«ï¼‰
    model.print_trainable_parameters()

    # ===================== 4. åŠ è½½å¹¶æ ¼å¼åŒ–æ•°æ® =====================
    def format_example(example):
        """å°†æ•°æ®æ ¼å¼åŒ–ä¸ºQwençš„å¯¹è¯æ ¼å¼ï¼Œå¹¶è¿›è¡ŒMaskingå’Œæˆªæ–­å¤„ç†"""
        # 1. åˆ†åˆ«å¤„ç†Inputå’ŒOutput
        user_text = f"<|im_start|>user\n{example['input']}<|im_end|>\n<|im_start|>assistant\n"
        assistant_text = f"{example['output']}<|im_end|>"
        
        # 2. Tokenize
        user_tokens = tokenizer(user_text, add_special_tokens=False)["input_ids"]
        assistant_tokens = tokenizer(assistant_text, add_special_tokens=False)["input_ids"]
        
        # 3. åŠ¨æ€æˆªæ–­ï¼ˆä¿ç•™å®Œæ•´çš„å›å¤ï¼Œæˆªæ–­è¿‡é•¿çš„è¾“å…¥ï¼‰
        # é¢„ç•™ç»™å›å¤çš„é•¿åº¦
        max_seq_length = 2048  # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦åˆ°2048
        if len(assistant_tokens) > max_seq_length:
            # å¦‚æœå›å¤æœ¬èº«å°±è¶…é•¿ï¼Œå¼ºåˆ¶æˆªæ–­å›å¤ï¼ˆæç«¯æƒ…å†µï¼‰
            assistant_tokens = assistant_tokens[:max_seq_length]
            user_tokens = []
        else:
            # å‰©ä½™ç©ºé—´ç»™è¾“å…¥
            remaining_length = max_seq_length - len(assistant_tokens)
            if len(user_tokens) > remaining_length:
                # æˆªæ–­è¾“å…¥ï¼ˆä¿ç•™åé¢çš„éƒ¨åˆ†é€šå¸¸æ›´æœ‰ç”¨ï¼Œæˆ–è€…ä¿ç•™å‰é¢çš„éƒ¨åˆ†ï¼‰
                # è¿™é‡Œé€‰æ‹©ä¿ç•™å‰é¢çš„éƒ¨åˆ†ï¼ˆOCRæ–‡æ¡£é€šå¸¸å¼€å¤´åŒ…å«å…³é”®ä¿¡æ¯ï¼‰ï¼Œå¦‚æœéœ€è¦ä¿ç•™æœ«å°¾æ”¹ä¸º user_tokens[-remaining_length:]
                user_tokens = user_tokens[:remaining_length]
        
        # 4. æ‹¼æ¥
        input_ids = user_tokens + assistant_tokens
        attention_mask = [1] * len(input_ids)
        
        # 5. æ„å»ºLabelsï¼ˆMaskæ‰Useréƒ¨åˆ†ï¼‰
        # Useréƒ¨åˆ†è®¾ä¸º-100ï¼ˆä¸è®¡ç®—Lossï¼‰ï¼ŒAssistantéƒ¨åˆ†ä¿ç•™åŸID
        labels = [-100] * len(user_tokens) + assistant_tokens
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    
    # åŠ è½½JSONLæ•°æ®
    dataset = load_dataset("json", data_files=DATA_PATH)["train"]
    # æ ¼å¼åŒ–æ•°æ®
    dataset = dataset.map(format_example, remove_columns=dataset.column_names)

    # ===================== 5. é…ç½®è®­ç»ƒå‚æ•°ï¼ˆMacä¸“ç”¨ï¼Œæ”¯æŒCheckpointï¼‰ =====================
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=3,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,  # æ¢¯åº¦ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿbatch_size=8ï¼Œç¨³å®šè®­ç»ƒ
        learning_rate=3e-4,        
        logging_steps=2,           
        # æ ¸å¿ƒä¿®æ”¹1ï¼šå¼€å¯checkpointä¿å­˜
        save_steps=50,            # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡checkpointï¼ˆå¯æ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼‰
        save_total_limit=3,       # åªä¿ç•™æœ€è¿‘3ä¸ªcheckpointï¼Œé¿å…å æ»¡ç£ç›˜
        save_strategy="steps",    # æŒ‰æ­¥æ•°ä¿å­˜ï¼ˆæ›¿ä»£åŸæ¥çš„"no"ï¼‰
        use_cpu=True,             
        fp16=False,                
        gradient_checkpointing=False,  
        report_to="none",          
        remove_unused_columns=False,
        # æ ¸å¿ƒä¿®æ”¹2ï¼šå¼€å¯æ–­ç‚¹ç»­è·‘çš„å…³é”®å‚æ•°
        load_best_model_at_end=False,  # ä¸éœ€è¦åŠ è½½æœ€ä¼˜æ¨¡å‹ï¼ˆLoRAå¾®è°ƒåœºæ™¯ï¼‰
    )

    # æ•°æ®æ•´ç†å™¨ï¼ˆè‡ªåŠ¨paddingï¼‰
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True,
        return_tensors="pt"
    )

    # ===================== 6. å¯åŠ¨è®­ç»ƒï¼ˆæ”¯æŒç»­è·‘ï¼‰ =====================
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
    )
    
    # è‡ªåŠ¨æ£€æµ‹æœ€æ–°çš„checkpoint
    if RESUME_FROM_CHECKPOINT is None and os.path.exists(OUTPUT_DIR):
        checkpoints = [f for f in os.listdir(OUTPUT_DIR) if f.startswith("checkpoint-")]
        if checkpoints:
            latest_checkpoint_name = max(checkpoints, key=lambda x: int(x.split("-")[1]))
            RESUME_FROM_CHECKPOINT = os.path.join(OUTPUT_DIR, latest_checkpoint_name)
            print(f"ğŸ” æ£€æµ‹åˆ°æœ€æ–°checkpointï¼š{RESUME_FROM_CHECKPOINT}ï¼Œå°†ä»è¯¥ä½ç½®ç»­è·‘")
    
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼Œ650æ¡æ•°æ®ï¼ŒMac CPUï¼‰")
    trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)

    # ===================== 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹ =====================
    # ä¿å­˜æœ€ç»ˆçš„LoRAæ¨¡å‹ï¼ˆåŒ…å«æ‰€æœ‰è®­ç»ƒå‚æ•°ï¼‰
    final_model_dir = os.path.join(OUTPUT_DIR, "final_model")
    model.save_pretrained(final_model_dir)
    tokenizer.save_pretrained(final_model_dir)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°ï¼š{final_model_dir}")
    print(f"ğŸ“Œ Checkpointæ–‡ä»¶ä¿å­˜åœ¨ï¼š{OUTPUT_DIR}ï¼ˆä»¥checkpoint-å¼€å¤´çš„æ–‡ä»¶å¤¹ï¼‰")

# ç®€åŒ–ç‰ˆprepare_model_for_kbit_trainingï¼ˆé€‚é…CPUï¼‰
def prepare_model_for_lora_training(model):
    for param in model.parameters():
        param.requires_grad = False  # å†»ç»“åŸºåº§æ¨¡å‹
        if param.ndim == 1:
            # é¿å…æ¢¯åº¦æº¢å‡º
            param.data = param.data.to(torch.float32)
    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()
    return model

if __name__ == "__main__":
    main()
    