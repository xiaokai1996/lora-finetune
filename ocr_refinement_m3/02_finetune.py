#!/Users/bytedance/codes/lora-finetune/.venv/bin/python3
"""
åŠŸèƒ½ï¼šåŠ è½½Qwen-1.5-1.8Bï¼Œç”¨100æ¡æ•°æ®åšLoRAå¾®è°ƒï¼Œä¿å­˜æ¨¡å‹åˆ°mac_demo/
æ‰§è¡Œæ–¹å¼ï¼špython3 02_finetune.py
ä¾èµ–ï¼šéœ€å…ˆæ‰§è¡Œ01_generate_data.pyç”Ÿæˆæ•°æ®ï¼Œä¸”å®‰è£…ä¾èµ–ï¼ˆpip3 install torch transformers datasets peft accelerateï¼‰
"""
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model

def main():
    # ===================== 1. é…ç½®å‚æ•°ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰ =====================
    MODEL_NAME = "Qwen/Qwen1.5-1.8B"
    DATA_PATH = "/Users/bytedance/codes/lora-finetune/ocr_refinement_m3/data/650case.jsonl"
    OUTPUT_DIR = "/Users/bytedance/codes/lora-finetune/ocr_refinement_m3/model_finetuned_output"

    # ===================== 2. åŠ è½½æ¨¡å‹å’ŒTokenizer =====================
    print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½Qwen-1.8Bï¼Œçº¦1.8GBï¼‰")
    # Mac CPUä¸“ç”¨é…ç½®ï¼šä¸é‡åŒ–ã€float32ã€device_map=cpu
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        torch_dtype=torch.float32,
        device_map="cpu"
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        padding_side="right"  # å³å¡«å……ï¼Œé¿å…å½±å“ç”Ÿæˆ
    )
    # Qwené»˜è®¤æ— pad_tokenï¼Œæ‰‹åŠ¨è®¾ç½®ä¸ºeos_token
    tokenizer.pad_token = tokenizer.eos_token

    # ===================== 3. é…ç½®LoRAï¼ˆæç®€ç‰ˆï¼ŒåŠ å¿«è®­ç»ƒï¼‰ =====================
    model = prepare_model_for_lora_training(model)  # ç®€åŒ–ç‰ˆprepareï¼Œé€‚é…CPU
    # lora_config = LoraConfig(
    #     r=4,                # ä½ç§©ï¼Œå‡å°‘å‚æ•°
    #     lora_alpha=16,      # ç¼©æ”¾å› å­
    #     lora_dropout=0.05,  # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
    #     target_modules=["q_proj", "v_proj"],  # è®­ç»ƒq_projå’Œv_projï¼Œé€‚é…Qwen1.5
    #     bias="none",
    #     task_type="CAUSAL_LM"  # å› æœè¯­è¨€æ¨¡å‹ï¼Œé€‚é…Qwen
    # )

    # é€‚é…å°æ ·æœ¬650æ¡é«˜è´¨é‡æ•°æ®è®­ç»ƒå‚æ•°
    lora_config = LoraConfig(
        r=2,                
        lora_alpha=8,      # ç¼©æ”¾å› å­
        lora_dropout=0.01,  # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        target_modules=["q_proj", "v_proj"],  
        bias="none",
        task_type="CAUSAL_LM"  # å› æœè¯­è¨€æ¨¡å‹ï¼Œé€‚é…Qwen
    )

    model = get_peft_model(model, lora_config)
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ï¼ˆçº¦50ä¸‡ï¼Œæå¿«ï¼‰
    model.print_trainable_parameters()

    # ===================== 4. åŠ è½½å¹¶æ ¼å¼åŒ–æ•°æ® =====================
    def format_example(example):
        """å°†æ•°æ®æ ¼å¼åŒ–ä¸ºQwençš„å¯¹è¯æ ¼å¼"""
        prompt = f"<|im_start|>user\n{example['input']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"
        # Tokenize
        tokenized = tokenizer(
            prompt,
            truncation=True,
            max_length=512,  # é™åˆ¶é•¿åº¦
            padding=False,   # äº¤ç»™DataCollatorå¤„ç†padding
            return_tensors=None
        )
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized
    
    # åŠ è½½JSONLæ•°æ®
    dataset = load_dataset("json", data_files=DATA_PATH)["train"]
    # æ ¼å¼åŒ–æ•°æ®
    dataset = dataset.map(format_example, remove_columns=dataset.column_names)

    # ===================== 5. é…ç½®è®­ç»ƒå‚æ•°ï¼ˆMacä¸“ç”¨ï¼‰ =====================
    # training_args = TrainingArguments(
    #     output_dir=OUTPUT_DIR,
    #     num_train_epochs=EPOCHS,
    #     per_device_train_batch_size=BATCH_SIZE,
    #     learning_rate=3e-4,        # è¾ƒå¤§çš„å­¦ä¹ ç‡ï¼ŒåŠ å¿«æ”¶æ•›
    #     logging_steps=2,           # æ¯2æ­¥æ‰“å°æ—¥å¿—ï¼Œå¿«é€Ÿçœ‹è¿›åº¦
    #     save_steps=1000,           # ä»…100æ¡æ•°æ®ï¼Œæ— éœ€é¢‘ç¹ä¿å­˜
    #     use_cpu=True,              # å¼ºåˆ¶ä½¿ç”¨CPU
    #     fp16=False,                # Mac CPUä¸æ”¯æŒFP16
    #     gradient_checkpointing=False,  # å…³é—­ï¼ŒèŠ‚çœå†…å­˜
    #     report_to="none",          # ä¸ä½¿ç”¨wandbï¼Œé¿å…ä¾èµ–
    #     remove_unused_columns=False,
    #     save_strategy="no"         # ä»…ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ŒåŠ å¿«é€Ÿåº¦
    # )

    # 650æ¡é«˜è´¨é‡æ•°æ®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=1,                # ä»…1è½®
        per_device_train_batch_size=8,     # æœ€å¤§æ‰¹æ¬¡ï¼Œæ€»æ­¥æ•°ä»…82æ­¥
        learning_rate=5e-4,                # å¤§å­¦ä¹ ç‡ï¼ŒåŠ å¿«æ”¶æ•›
        warmup_ratio=0.0,                  # å…³é—­é¢„çƒ­ï¼Œå‡å°‘è®¡ç®—
        lr_scheduler_type="constant",      # å›ºå®šå­¦ä¹ ç‡ï¼Œæœ€å¿«

        # æ—¥å¿—/ä¿å­˜ï¼šæè‡´å‡å°‘IOè€—æ—¶
        logging_steps=50,                  # æ¯50æ­¥æ‰“å°1æ¬¡ï¼ˆåŸ2æ­¥å¤ªé¢‘ç¹ï¼‰
        save_steps=1000,
        save_strategy="no",                # ä»…è®­ç»ƒç»“æŸä¿å­˜

        # Mac CPUå…³é”®é…ç½®ï¼ˆæ”¯æ’‘batch=8ï¼‰
        use_cpu=True,
        fp16=False,
        gradient_checkpointing=True,       # å¿…é¡»å¼€ï¼èŠ‚çœ50%å†…å­˜
        report_to="none",
        remove_unused_columns=False,
    )

    # æ•°æ®æ•´ç†å™¨ï¼ˆè‡ªåŠ¨paddingï¼‰
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True,
        return_tensors="pt"
    )

    # ===================== 6. å¯åŠ¨è®­ç»ƒ =====================
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator
    )
    
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒï¼ˆ650æ¡æ•°æ®ï¼Œ1è½®ï¼ŒMac CPUçº¦2-5åˆ†é’Ÿï¼‰")
    trainer.train()

    # ===================== 7. ä¿å­˜æ¨¡å‹ =====================
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°ï¼š{OUTPUT_DIR}")

# ç®€åŒ–ç‰ˆprepare_model_for_kbit_trainingï¼ˆé€‚é…CPUï¼‰
def prepare_model_for_lora_training(model):
    for param in model.parameters():
        param.requires_grad = False  # å†»ç»“åŸºåº§æ¨¡å‹
        if param.ndim == 1:
            # é¿å…æ¢¯åº¦æº¢å‡º
            param.data = param.data.to(torch.float32)
    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()
    return model

if __name__ == "__main__":
    main()
